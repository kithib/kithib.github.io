---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a third-year Master's student at the School of Computer and Software, **Shenzhen University**, supervised by Prof. [Fei Richard Yu](https://scholar.google.com/citations?user=zuGMGBoAAAAJ). I am also fortunate to be advised by Prof. [Yao Shu](https://yao.notion.site/). 

My research interests primarily lie in the post-training of Large Language Models (LLMs), including Supervised Fine-Tuning (SFT), Prompt Optimization, and Reinforcement Learning. Additionally, I am interested in learning theories, such as machine learning theory and reinforcement learning theory. 

If you are interested in my research, please feel free to contact me via [![Email](https://img.shields.io/badge/Email-Contact_Me-blue?style=flat&logo=gmail&logoColor=white)](mailto:weichenxing2023@email.szu.edu.cn).

<br/>
# üî• News


<div style="max-height: 200px; overflow-y: auto;">
<ul>

  <li><em>2025.12:</em> üéâüéâ <a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/115973">ReDit</a> is accepted by <strong>NeurIPS 2025</strong>.</li>

  <li><em>2025.11:</em> üéâüéâ <a href="https://dl.acm.org/doi/abs/10.1145/3746027.3758269">UniSVG</a> is accepted by <strong>ACM MM 2025 Dataset Track</strong>.</li>

  <li><em>2025.11:</em> üéâüéâ <a href="https://aclanthology.org/2025.emnlp-main.37/">PAFT</a> is accepted by <strong>EMNLP 2025</strong>, and wins the <strong>SAC Highlights Award</strong> (TOP 2%) at <strong>EMNLP 2025!</strong> </li>

  <li><em>2025.10:</em> I serve as a reviewer for <strong>ICLR 2026</strong>.</li>

  <li><em>2025.10:</em> We propose <a href="https://arxiv.org/abs/2510.21830">GAPO</a>, a method that robustly handles skewed reward distributions with outliers in code-editing RL by adaptively computing advantages, leading to consistent performance improvements. Check our <a href="https://github.com/TsingZ0/verl-GAPO"> Github</a>.</li>

  <li><em>2025.10:</em> We propose <a href="https://arxiv.org/abs/2510.24832">R-Score</a>, a novel metric to quantify the learnability of queries in RL to enhenced the curriculum learning method. Check our <a href="https://github.com/zz-haooo/Re-Schedule"> Github</a>.</li>

  <li><em>2025.09:</em> We propose <a href="https://arxiv.org/abs/2509.23166">ROSA</a>, a lightweight algorithm for our test-time adaptation paradigm that enables LLMs to perform efficient in-conversation self-correction by updating parameters online using real-time user feedback. Check our <a href="https://github.com/kithib/ROSA"> Github</a>.</li>

  <li><em>2025.08:</em> We propose <a href="https://arxiv.org/abs/2508.07766">UniSVG</a>, a SVG dataset for improving MLLM SVG generate performance. Check our <a href="https://ryanlijinke.github.io/"> Project Page</a> and <a href="https://huggingface.co/datasets/lili24/UniSVG"> Hugging Face</a>.</li>


  <li><em>2025.06:</em> We propose <a href="https://arxiv.org/abs/2506.18631">ReDit</a>, a technique that enhances reinforcement learning in large language models by adding random perturbations to reward signals, improving training efficiency and convergence speed while maintaining performance. Check our <a href="https://github.com/kithib/ReDit"> Github</a>.</li>

  <li><em>2025.06:</em> üéâüéâ <a href="https://aclanthology.org/2025.acl-long.713/">Flexora</a> is accepted by <strong>ACL 2025</strong>.</li>

  <li><em>2025.02:</em> We propose <a href="https://arxiv.org/abs/2502.12859">PAFT</a>, which dynamically adjusts prompts during training, improving robustness, generalization, and even inference speed. Check our <a href="https://github.com/kithib/PAFT"> Github</a>.</li>

  <li><em>2024.08:</em> We propose <a href="https://arxiv.org/abs/2408.10774">Flexora</a>, a novel method that enhances Large Language Model fine-tuning efficiency by selectively adapting only the most critical layers. Check our <a href="https://github.com/kithib/Flexora"> Github</a>.</li>

  <li><em>2024.02:</em> We propose <a href="https://arxiv.org/abs/2402.18679">Data Interpreter</a>, an LLM agent for solving data science problems. Check our <a href=" https://github.com/geekan/MetaGPT"> Github</a>.</li>
  
</ul>
</div>

<br/>
# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# üé° Service
<!-- ## Conference Committee Member -->
- Reviewer for ICLR'2026

# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- Shenzhen University<img src="images/insignia/szu.jpg" alt="SZU Insignia" style="float: right; width: 90px;"/>

  PhD, Computer Science, 2023.09 - (now), 

  Advisor: Prof. [Fei Richard Yu](https://scholar.google.com/citations?user=zuGMGBoAAAAJ)
  Co-Advisor: Prof. [Yao Shu](https://yao.notion.site/)
<br>
<br>

<!--
# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)
-->

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
